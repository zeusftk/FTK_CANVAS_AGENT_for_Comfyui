import os
import torch
import torchaudio
import numpy as np
from PIL import Image, ImageDraw, ImageFont, ImageOps
import cv2
import subprocess
import folder_paths
import folder_paths


# å®šä¹‰ä¸€ä¸ªå°†tensorè½¬æ¢ä¸ºPILå›¾åƒçš„å‡½æ•°ã€‚
def tensor2PIL(tensor, thr=0.5):
    if tensor is None:
        return None
    tensor = tensor.squeeze()
    if tensor.dtype == torch.float32:
        tensor = tensor * 255
    if tensor.ndim == 3:
        tensor = tensor.permute(1, 2, 0)
    if tensor.shape[2] == 1:
        tensor = tensor.squeeze(2)
    tensor = tensor.cpu().numpy().astype(np.uint8)
    if tensor.ndim == 2:
        return Image.fromarray(tensor, mode='L')
    elif tensor.ndim == 3:
        if tensor.shape[2] == 3:
            return Image.fromarray(tensor, mode='RGB')
        elif tensor.shape[2] == 4:
            return Image.fromarray(tensor, mode='RGBA')
    return None

# å®šä¹‰ä¸€ä¸ªå°†PILå›¾åƒè½¬æ¢ä¸ºtensorçš„å‡½æ•°
def PIL2tensor(pil_image):
    if pil_image is None:
        return None
    if pil_image.mode == 'RGB':
        arr = np.array(pil_image)
        arr = arr[:, :, ::-1]
        arr = arr.astype(np.float32) / 255.0
        tensor = torch.from_numpy(arr)
        tensor = tensor.permute(2, 0, 1)
    elif pil_image.mode == 'RGBA':
        arr = np.array(pil_image)
        r, g, b, a = arr[:, :, 0], arr[:, :, 1], arr[:, :, 2], arr[:, :, 3]
        arr = np.stack((r, g, b, a), axis=0)
        arr = arr.astype(np.float32) / 255.0
        tensor = torch.from_numpy(arr)
    elif pil_image.mode == 'L':
        arr = np.array(pil_image)
        arr = arr.astype(np.float32) / 255.0
        tensor = torch.from_numpy(arr)
        tensor = tensor.unsqueeze(0)
    else:
        pil_image = pil_image.convert('RGB')
        arr = np.array(pil_image)
        arr = arr[:, :, ::-1]
        arr = arr.astype(np.float32) / 255.0
        tensor = torch.from_numpy(arr)
        tensor = tensor.permute(2, 0, 1)
    return tensor

# ç”Ÿæˆè‰ºæœ¯æ–‡å­—å›¾åƒçš„å‡½æ•°
def generate_art_text_image(text, font_size, font_color, bg_color, width, height, align='center'):
    # åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒ
    image = Image.new('RGBA', (width, height), bg_color)
    draw = ImageDraw.Draw(image)
    
    # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
    try:
        font = ImageFont.truetype('simhei.ttf', font_size)
    except:
        # å¦‚æœæ‰¾ä¸åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
        try:
            font = ImageFont.load_default()
        except:
            font = None
    
    # åˆå§‹åŒ–é»˜è®¤å€¼
    x, y = 0, 0
    
    # è®¡ç®—æ–‡æœ¬çš„å°ºå¯¸
    if font:
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
    else:
        text_width, text_height = 100, 50  # å‡è®¾çš„é»˜è®¤å¤§å°
    
    # è®¡ç®—æ–‡æœ¬çš„ä½ç½®ä»¥å®ç°å±…ä¸­å¯¹é½
    if align == 'center':
        x = (width - text_width) // 2
        y = (height - text_height) // 2
    elif align == 'left':
        x = 10
        y = (height - text_height) // 2
    elif align == 'right':
        x = width - text_width - 10
        y = (height - text_height) // 2
    
    # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ–‡æœ¬
    if font:
        draw.text((x, y), text, font=font, fill=font_color)
    else:
        draw.text((x, y), text, fill=font_color)
    
    return image

class FTKSaveImage:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "images": ("IMAGE", {"default": None}),
            "output_dir": ("STRING", {"default": '', "multiline": False, "tooltip": "è¾“å‡ºç›®å½•"}),
            "filename_prefix": ("STRING", {"default": "FTK"}),
            }}

    CATEGORY = "ğŸµï¸ FTK/I_O"
    RETURN_TYPES = ("FTKOUT",)
    RETURN_NAMES = ("file_path",)
    FUNCTION = "save_images"
    OUTPUT_NODE = True

    def save_images(self, images, output_dir='', filename_prefix="FTK"):
        results = []
        paths=[]
        if images is None:
            return results
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨        
        if output_dir=='':
            output_dir=folder_paths.get_output_directory()
        os.makedirs(output_dir, exist_ok=True)
        
        #å›¾ç‰‡åæ ¼å¼filename_prefix_00000.png è·å–ç›®å½•ä¸‹pngæ–‡ä»¶ååŒ¹é…æœ€å6ä½å¦‚æœå­˜åœ¨åˆ™ä»æœ€å6ä½å¼€å§‹é€’å¢
        existing_files = [f for f in os.listdir(output_dir) if f.endswith('.png') and f.startswith(filename_prefix)]
        if existing_files:
            existing_numbers = []
            for f in existing_files:
                # å°è¯•æå–æ•°å­—éƒ¨åˆ†
                try:
                    number_part = f.split('_')[-1].split('.')[0]
                    if number_part.isdigit():
                        existing_numbers.append(int(number_part))
                except:
                    continue
            
            if existing_numbers:
                next_number = max(existing_numbers) + 1
            else:
                next_number = 0
        else:
            next_number = 0


        # å¤„ç†æ¯ä¸ªå›¾åƒ
        for i, image in enumerate(images):
            # ç¡®ä¿å›¾åƒæ˜¯æ­£ç¡®çš„æ ¼å¼
            if isinstance(image, torch.Tensor):
                # å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ
                img_np = image.cpu().numpy()
                img_np = (img_np * 255).astype(np.uint8)
                
                # å¤„ç†å›¾åƒç»´åº¦
                if len(img_np.shape) == 4:
                    img_np = img_np[0]
                if img_np.shape[0] == 1 or img_np.shape[0] == 3 or img_np.shape[0] == 4:
                    img_np = np.transpose(img_np, (1, 2, 0))
                
                # åˆ›å»ºPILå›¾åƒ
                if img_np.shape[2] == 1:
                    img_pil = Image.fromarray(img_np[:, :, 0], mode="L")
                elif img_np.shape[2] == 3:
                    img_pil = Image.fromarray(img_np, mode="RGB")
                elif img_np.shape[2] == 4:
                    img_pil = Image.fromarray(img_np, mode="RGBA")
                else:
                    img_pil = Image.fromarray(img_np)
            else:
                # å‡è®¾å·²ç»æ˜¯PILå›¾åƒ
                img_pil = image
            
            # ç”Ÿæˆæ–‡ä»¶å - ä½¿ç”¨next_numberå¹¶æ ¼å¼åŒ–ä¸º5ä½æ•°å­—
            current_image_number = next_number + i
            filename = f"{filename_prefix}_{current_image_number:05d}.png"
            file_path = os.path.join(output_dir, filename)
            
            # ä¿å­˜å›¾åƒ
            img_pil.save(file_path)
            
            # æ·»åŠ åˆ°ç»“æœä¸­
            results.append({
                "filename": filename,
                "subfolder": "",
                "type": "output"
            })
            paths.append(file_path)
        
        return {"result": (paths,),"ui":{"images": results}}

class FTKSaveVideo:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "frames": ("IMAGE", {"default": None}),
                "output_dir": ("STRING", {"default": '', "multiline": False, "tooltip": "è¾“å‡ºç›®å½•"}),
                "filename_prefix": ("STRING", {"default": "FTK"}),
                "fps": ("FLOAT", {"default": 25, "min": 1, "max": 60, "step": 0.1}),
            },
            "optional": {
                "audio": ("AUDIO", {"default": None, "tooltip": "å¯é€‰çš„éŸ³é¢‘è¾“å…¥ï¼Œæä¾›æ—¶å°†åˆå¹¶åˆ°è§†é¢‘ä¸­"}),
            }
        }

    CATEGORY = "ğŸµï¸ FTK/I_O"
    RETURN_TYPES = ("FTKOUT",)
    RETURN_NAMES = ("file_path",)
    FUNCTION = "save_video"
    OUTPUT_NODE = True

    def save_video(self, frames, output_dir='', filename_prefix="FTK", fps=24, audio=None):
        results = []
        paths = []
        if frames is None:
            return results
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if output_dir=='':
            output_dir=folder_paths.get_output_directory()
        os.makedirs(output_dir, exist_ok=True)
        
        #è§†é¢‘åæ ¼å¼filename_prefix_00000.mp4 è·å–ç›®å½•ä¸‹mp4æ–‡ä»¶ååŒ¹é…æœ€å6ä½å¦‚æœå­˜åœ¨åˆ™ä»æœ€å6ä½å¼€å§‹é€’å¢
        existing_files = [f for f in os.listdir(output_dir) if f.endswith('.mp4') and f.startswith(filename_prefix)]
        if existing_files:
            existing_numbers = []
            for f in existing_files:
                # å°è¯•æå–æ•°å­—éƒ¨åˆ†
                try:
                    number_part = f.split('_')[-1].split('.')[0]
                    if number_part.isdigit():
                        existing_numbers.append(int(number_part))
                except:
                    continue
            
            if existing_numbers:
                next_number = max(existing_numbers) + 1
            else:
                next_number = 0
        else:
            next_number = 0
        
        # ç”Ÿæˆæ–‡ä»¶å - ä½¿ç”¨next_numberå¹¶æ ¼å¼åŒ–ä¸º5ä½æ•°å­—ï¼Œç¡®ä¿ä½¿ç”¨.mp4æ‰©å±•å
        filename = f"{filename_prefix}_{next_number:05d}.mp4"
        file_path = os.path.join(output_dir, filename)
        
        # å¤„ç†frames
        if isinstance(frames, torch.Tensor):
            # ç¡®ä¿framesæ˜¯æ­£ç¡®çš„æ ¼å¼
            if len(frames.shape) == 5:  # æ‰¹æ¬¡ç»´åº¦
                frames = frames[0]
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            frames_np = frames.cpu().numpy()
            
            # å¤„ç†ç»´åº¦å’Œæ•°æ®ç±»å‹
            if frames_np.shape[-1] == 3:  # [frames, height, width, 3]
                # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
                if frames_np.max() <= 1.0:
                    frames_np = (frames_np * 255).astype(np.uint8)
                else:
                    frames_np = frames_np.astype(np.uint8)
                
                # è½¬æ¢ä¸ºBGRæ ¼å¼
                frames_np = frames_np[..., ::-1]  # RGB to BGR
                
                # è·å–è§†é¢‘å°ºå¯¸
                height, width = frames_np.shape[1], frames_np.shape[2]
            else:
                # å¦‚æœç»´åº¦ä¸æ­£ç¡®ï¼Œä½¿ç”¨é»˜è®¤å°ºå¯¸
                height, width = 512, 512
                frames_np = np.zeros((len(frames_np), height, width, 3), dtype=np.uint8)
        else:
            # å‡è®¾framesæ˜¯ä¸€ä¸ªåˆ—è¡¨
            if not frames:
                return results
            
            # è·å–ç¬¬ä¸€å¸§çš„å°ºå¯¸
            first_frame = frames[0]
            if isinstance(first_frame, torch.Tensor):
                first_frame_np = first_frame.cpu().numpy()
                if first_frame_np.max() <= 1.0:
                    first_frame_np = (first_frame_np * 255).astype(np.uint8)
                height, width = first_frame_np.shape[0], first_frame_np.shape[1]
            else:
                height, width = first_frame.shape[0], first_frame.shape[1]
            
            # è½¬æ¢æ‰€æœ‰å¸§
            frames_np = []
            for frame in frames:
                if isinstance(frame, torch.Tensor):
                    frame_np = frame.cpu().numpy()
                    if frame_np.max() <= 1.0:
                        frame_np = (frame_np * 255).astype(np.uint8)
                    frame_np = frame_np[..., ::-1]  # RGB to BGR
                else:
                    frame_np = frame[..., ::-1]  # RGB to BGR
                frames_np.append(frame_np)
            
            frames_np = np.array(frames_np)
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºå­˜å‚¨æœªå‹ç¼©çš„è§†é¢‘å¸§ï¼Œç¡®ä¿ä½¿ç”¨.mp4æ‰©å±•å
        temp_raw_path = os.path.join(output_dir, f"{filename_prefix}_raw_{next_number:05d}.mp4")
        
        if audio is None:
            # æ²¡æœ‰éŸ³é¢‘æ—¶ï¼Œç›´æ¥ä½¿ç”¨ffmpegä¿å­˜ä¸ºH.264ç¼–ç çš„è§†é¢‘
            # ä½¿ç”¨ffmpegå‘½ä»¤è¡Œå·¥å…·å¤„ç†ï¼Œç¡®ä¿ä½¿ç”¨H.264ç¼–ç 
            # å…ˆå°†å¸§ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åç”¨ffmpegè½¬æ¢
            
            # ä½¿ç”¨OpenCVå…ˆä¿å­˜ä¸ºä¸´æ—¶rawè§†é¢‘
            # å®šä¹‰è§†é¢‘ç¼–ç å™¨ - ä½¿ç”¨MJPGä½œä¸ºä¸­é—´æ ¼å¼
            fourcc_temp = cv2.VideoWriter_fourcc(*'mp4v')
            temp_out = cv2.VideoWriter(temp_raw_path, fourcc_temp, fps, (width, height))
            
            # å†™å…¥æ¯ä¸€å¸§åˆ°ä¸´æ—¶æ–‡ä»¶
            for frame in frames_np:
                temp_out.write(frame)
            
            # é‡Šæ”¾ä¸´æ—¶VideoWriter
            temp_out.release()
            
            # ä½¿ç”¨ffmpegå°†ä¸´æ—¶è§†é¢‘è½¬æ¢ä¸ºH.264ç¼–ç çš„MP4
            ffmpeg_cmd = [
                'ffmpeg',
                '-i', temp_raw_path,
                '-c:v', 'libx264',  # ä½¿ç”¨H.264ç¼–ç 
                '-preset', 'medium',  # ç¼–ç é€Ÿåº¦å’Œå‹ç¼©ç‡çš„å¹³è¡¡
                '-crf', '23',  # æ’å®šé€Ÿç‡å› å­ï¼Œè¶Šä½è´¨é‡è¶Šå¥½
                '-c:a', 'aac',  # éŸ³é¢‘ç¼–ç ä¸ºAAC
                '-strict', 'experimental',
                '-y',  # è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
                file_path
            ]
            
            # æ‰§è¡Œffmpegå‘½ä»¤
            subprocess.run(ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_raw_path):
                os.remove(temp_raw_path)
        else:
            # æœ‰éŸ³é¢‘æ—¶çš„å¤„ç†æµç¨‹
            # ä¿å­˜æ— å£°è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç¡®ä¿ä½¿ç”¨.mp4æ‰©å±•å
            temp_video_path = os.path.join(output_dir, f"{filename_prefix}_temp_{next_number:05d}.mp4")
            
            # ä½¿ç”¨MJPGä½œä¸ºä¸­é—´æ ¼å¼ä¿å­˜ä¸´æ—¶è§†é¢‘
            fourcc_temp = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_video_path, fourcc_temp, fps, (width, height))
            
            # å†™å…¥æ¯ä¸€å¸§
            for frame in frames_np:
                out.write(frame)
            
            # é‡Šæ”¾VideoWriter
            out.release()
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶wavæ–‡ä»¶
            temp_audio_path = os.path.join(output_dir, f"{filename_prefix}_temp_{next_number:05d}.wav")
            # å¤„ç†éŸ³é¢‘æ•°æ®
            for (batch_number, waveform) in enumerate(audio["waveform"].cpu()):
                torchaudio.save(temp_audio_path, waveform, audio["sample_rate"], format="wav")
                break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„éŸ³é¢‘
            
            # ä½¿ç”¨ffmpegåˆå¹¶è§†é¢‘å’ŒéŸ³é¢‘ï¼Œå¹¶ç¡®ä¿ä½¿ç”¨H.264ç¼–ç 
            ffmpeg_cmd = [
                'ffmpeg',
                '-i', temp_video_path,
                '-i', temp_audio_path,
                '-c:v', 'libx264',  # ä½¿ç”¨H.264ç¼–ç è§†é¢‘
                '-preset', 'medium',
                '-crf', '23',
                '-c:a', 'aac',   # éŸ³é¢‘ç¼–ç ä¸ºAAC
                '-strict', 'experimental',
                '-y',  # è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
                file_path
            ]
            
            # æ‰§è¡Œffmpegå‘½ä»¤
            subprocess.run(ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_video_path):
                os.remove(temp_video_path)
            if os.path.exists(temp_audio_path):
                os.remove(temp_audio_path)
        
        # æ·»åŠ åˆ°ç»“æœä¸­
         # æ·»åŠ åˆ°ç»“æœä¸­
        results.append({
            "filename": os.path.basename(file_path),
            "subfolder": "",
            "type": "output"
        })
        paths.append(file_path)
        return {"result": (paths,),"ui":{"gifs": results}}

class FTKSaveAudio:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {"audio": ("AUDIO", ),
                             "output_dir": ("STRING", {"default": '', "multiline": False, "tooltip": "è¾“å‡ºç›®å½•"}),
                            "filename_prefix": ("STRING", {"default": "FTK"})},
                }

    RETURN_TYPES = ("FTKOUT",)
    RETURN_NAMES = ("file_path",)
    FUNCTION = "save_audio"

    OUTPUT_NODE = True

    CATEGORY = "ğŸµï¸ FTK/I_O"

    def save_audio(self, audio, output_dir='', filename_prefix="FTK"):
        results = []
        paths = []
        if audio is None:
            return results
            
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if output_dir=='':
            output_dir=folder_paths.get_output_directory()
        os.makedirs(output_dir, exist_ok=True)
            
        #éŸ³é¢‘åæ ¼å¼filename_prefix_00000.wav è·å–ç›®å½•ä¸‹wavæ–‡ä»¶ååŒ¹é…æœ€å6ä½å¦‚æœå­˜åœ¨åˆ™ä»æœ€å6ä½å¼€å§‹é€’å¢
        existing_files = [f for f in os.listdir(output_dir) if f.endswith('.wav') and f.startswith(filename_prefix)]
        if existing_files:
            existing_numbers = []
            for f in existing_files:
                # å°è¯•æå–æ•°å­—éƒ¨åˆ†
                try:
                    number_part = f.split('_')[-1].split('.')[0]
                    if number_part.isdigit():
                        existing_numbers.append(int(number_part))
                except:
                    continue
            
            if existing_numbers:
                next_number = max(existing_numbers) + 1
            else:
                next_number = 0
        else:
            next_number = 0
            
        file_path = ""  # åˆå§‹åŒ–file_pathå˜é‡
        for (batch_number, waveform) in enumerate(audio["waveform"].cpu()):
            # ç”Ÿæˆæ–‡ä»¶å - ä½¿ç”¨next_numberå¹¶æ ¼å¼åŒ–ä¸º5ä½æ•°å­—
            current_number = next_number + batch_number
            file = f"{filename_prefix}_{current_number:05d}.wav"
            file_path = os.path.join(output_dir, file)
            torchaudio.save(file_path, waveform, audio["sample_rate"], format="wav")

            # æ·»åŠ åˆ°ç»“æœä¸­
             # æ·»åŠ åˆ°ç»“æœä¸­
            results.append({
                "filename": file,
                "subfolder": "",
                "type": "output"
            })
            paths.append(file_path)        
        return {"result": (paths,),"ui":{"audio": results}}

class FTKSaveText:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "text_content": ("STRING", {"forceInput": True}),
            "output_dir": ("STRING", {"default": '', "multiline": False, "tooltip": "è¾“å‡ºç›®å½•"}),
            "filename_prefix": ("STRING", {"default": "FTK"})},
        }

    RETURN_TYPES = ("FTKOUT",)
    RETURN_NAMES = ("file_path",)
    FUNCTION = "save_text"
    OUTPUT_NODE = True
    CATEGORY = "ğŸµï¸ FTK/I_O"

    def save_text(self, text_content, output_dir='', filename_prefix="FTK"):
        """ä¿å­˜æ–‡æœ¬å†…å®¹åˆ°æŒ‡å®šæ–‡ä»¶"""
        import os
        import folder_paths
        
        results = []
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if output_dir == '':
            output_dir = folder_paths.get_output_directory()
        os.makedirs(output_dir, exist_ok=True)
        
        # æ–‡ä»¶åæ ¼å¼: filename_prefix_00000.txt
        existing_files = [f for f in os.listdir(output_dir) if f.endswith('.txt') and f.startswith(filename_prefix)]
        max_number = -1
        
        if existing_files:
            for f in existing_files:
                try:
                    # å°è¯•æå–æ•°å­—éƒ¨åˆ†
                    number_part = f.split('_')[-1].split('.')[0]
                    if number_part.isdigit():
                        max_number = max(max_number, int(number_part))
                except:
                    pass
        
        # ç”Ÿæˆæ–°æ–‡ä»¶å
        new_number = max_number + 1
        filename = f"{filename_prefix}_{new_number:05d}.txt"
        full_path = os.path.join(output_dir, filename)
        
        # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(text_content)
        
        results.append(full_path)
        print(f"æ–‡æœ¬å·²ä¿å­˜åˆ°: {full_path}")
        
        return {"ui": {"text": (results,)}, "result": (results,)}

class FTK_OUTPUT:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "result": ("FTKOUT", {"forceInput": True}),
            }
    }
    RETURN_TYPES = ("STRING",)
    FUNCTION = "out"
    CATEGORY = "ğŸµï¸ FTK/I_O"
    OUTPUT_NODE = True 
    def out(self, result):
        #æ”¶åˆ°çš„FTKOUTç±»å‹æ•°æ®ï¼Œè§£æä¸ºList(file_path)
        print("FTKOUT--in>>:",result)
        if isinstance(result, list):
            out = "|".join(result)
        print("FTKOUT--out>>:",out)
        return {"ui": {"FTKOUTPUT": (result,)}, "result": (result,)}

class FTK_INPUT:
    """
    æ•´åˆèŠ‚ç‚¹ï¼Œåˆå¹¶äº†æ–‡æœ¬ã€å›¾ç‰‡ã€è§†é¢‘å’ŒéŸ³é¢‘è¾“å…¥åŠŸèƒ½
    ç‰¹ç‚¹ï¼š
    1. æ‰€æœ‰è¾“å…¥éƒ½æ˜¯å¯é€‰çš„
    2. æ·»åŠ äº†å®Œå–„çš„å¼‚å¸¸å¤„ç†
    3. æŒ‰åˆ†ç±»è°ƒæ•´äº†è¾“å…¥å‚æ•°çš„ä½ç½®
    4. ä¼˜åŒ–äº†å‘½åè§„èŒƒ
    5. æ”¯æŒä¸¤ä¸ªè§†é¢‘è¾“å…¥
    6. ç§»é™¤äº†scale_byå‚æ•°
    """
    @classmethod
    def INPUT_TYPES(s):
        
        return {
            "required": {
                # é€šç”¨è®¾ç½®
                "width": ("INT", {"default": 512, "min": 64, "max": 8192, "step": 8, "tooltip": "å›¾åƒå’Œè§†é¢‘çš„å®½åº¦"}),
                "height": ("INT", {"default": 512, "min": 64, "max": 8192, "step": 8, "tooltip": "å›¾åƒå’Œè§†é¢‘çš„é«˜åº¦"}),
                "length": ("INT", {"default": 48, "min": 16, "max": 1200, "step": 1, "tooltip": "é¢„æœŸè§†é¢‘çš„é•¿åº¦"}),
                "output_dir": ("STRING", {"default": '', "multiline": False, "tooltip": "è¾“å‡ºç›®å½•"}),
            },
            "optional": {
                # æ–‡æœ¬è¾“å…¥éƒ¨åˆ†
                "text_input_a": ("STRING", {"default": '', "multiline": True, "tooltip": "ä¸»è¦æ–‡æœ¬è¾“å…¥"}),
                "text_input_b": ("STRING", {"default": '', "multiline": True, "tooltip": "æ¬¡è¦æ–‡æœ¬è¾“å…¥"}),
                "text_input_c": ("STRING", {"default": '', "multiline": True, "tooltip": "ç¬¬ä¸‰æ–‡æœ¬è¾“å…¥"}),
                "text_input_d": ("STRING", {"default": '', "multiline": True, "tooltip": "ç¬¬å››æ–‡æœ¬è¾“å…¥"}),
                
                # å›¾ç‰‡è¾“å…¥éƒ¨åˆ†
                "image_1_path": ("STRING", {"default": '', "multiline": False, "tooltip": "ç¬¬ä¸€å¼ å›¾ç‰‡è·¯å¾„"}),
                "image_2_path": ("STRING", {"default": '', "multiline": False, "tooltip": "ç¬¬äºŒå¼ å›¾ç‰‡è·¯å¾„"}),
                "image_3_path": ("STRING", {"default": '', "multiline": False, "tooltip": "ç¬¬ä¸‰å¼ å›¾ç‰‡è·¯å¾„"}),
                
                # è§†é¢‘è¾“å…¥éƒ¨åˆ† - è§†é¢‘1
                "video_1_file": ("STRING", {"default": "", "multiline": False, "tooltip": "ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶è·¯å¾„"}),
                "video_1_frame_rate": ("FLOAT", {"default": 24, "min": 1, "max": 60, "step": 0.1, "tooltip": "ç¬¬ä¸€ä¸ªè§†é¢‘å¸§ç‡"}),
                "video_1_max_frames": ("INT", {"default": 64, "min": 1, "max": 10000, "step": 1, "tooltip": "ç¬¬ä¸€ä¸ªè§†é¢‘æœ€å¤§è¾“å‡ºå¸§æ•°"}),
                "video_1_start_frame": ("INT", {"default": 0, "min": 0, "step": 1, "tooltip": "ç¬¬ä¸€ä¸ªè§†é¢‘èµ·å§‹å¸§"}),
                
                # è§†é¢‘è¾“å…¥éƒ¨åˆ† - è§†é¢‘2
                "video_2_file": ("STRING", {"default": "", "multiline": False, "tooltip": "ç¬¬äºŒä¸ªè§†é¢‘æ–‡ä»¶è·¯å¾„"}),
                "video_2_frame_rate": ("FLOAT", {"default": 24, "min": 1, "max": 60, "step": 0.1, "tooltip": "ç¬¬äºŒä¸ªè§†é¢‘å¸§ç‡"}),
                "video_2_max_frames": ("INT", {"default": 64, "min": 1, "max": 10000, "step": 1, "tooltip": "ç¬¬äºŒä¸ªè§†é¢‘æœ€å¤§è¾“å‡ºå¸§æ•°"}),
                "video_2_start_frame": ("INT", {"default": 0, "min": 0, "step": 1, "tooltip": "ç¬¬äºŒä¸ªè§†é¢‘èµ·å§‹å¸§"}),
                
                # éŸ³é¢‘è¾“å…¥éƒ¨åˆ†
                "audio_main_path": ("STRING", {"default": "", "multiline": False, "tooltip": "ä¸»éŸ³é¢‘è·¯å¾„"}),
                "audio_alternate_path": ("STRING", {"default": "", "multiline": False, "tooltip": "å¤‡ç”¨éŸ³é¢‘è·¯å¾„"}),
            }
        }

    CATEGORY = "ğŸµï¸ FTK/I_O"
    
    # è¿”å›ç±»å‹åŒ…æ‹¬æ‰€æœ‰è¾“å…¥èŠ‚ç‚¹çš„è¿”å›ç±»å‹
    RETURN_TYPES = (
        # å¸¸ç”¨å‚æ•°
        "INT", "INT", "INT", "STRING",
        # æ–‡æœ¬è¿”å›
        "STRING", "STRING", "STRING", "STRING",
        # å›¾ç‰‡è¿”å›
        "IMAGE", "MASK", "IMAGE", "MASK", "IMAGE", "MASK",
        # è§†é¢‘è¿”å›
        "IMAGE",  # è§†é¢‘1
        "IMAGE",  # è§†é¢‘2
        # éŸ³é¢‘è¿”å›
        "AUDIO", "AUDIO"
    )
    
    RETURN_NAMES = (
        # å¸¸ç”¨å‚æ•°
        "width", "height", "length", "output_dir",
        # æ–‡æœ¬è¿”å›åç§°
        "text_a", "text_b", "text_c", "text_d",
        # å›¾ç‰‡è¿”å›åç§°
        "image_1", "mask_1", "image_2", "mask_2", "image_3", "mask_3",
        # è§†é¢‘è¿”å›åç§° - è§†é¢‘1
        "video_1_frames",
        # è§†é¢‘è¿”å›åç§° - è§†é¢‘2
        "video_2_frames",
        # éŸ³é¢‘è¿”å›åç§°
        "audio_1", "audio_2"
    )
    
    FUNCTION = "process_all_media"

    def load_single_image(self, image_path, width, height):
        """åŠ è½½å•å¼ å›¾ç‰‡"""
        try:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºç©º
            if not image_path or not image_path.strip():
                # è¿”å›é»˜è®¤å›¾ç‰‡å’Œé®ç½©
                default_image = torch.zeros((1, height, width, 3), dtype=torch.float32)
                default_mask = torch.zeros((height, width), dtype=torch.float32, device="cpu")
                return default_image, default_mask
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                print(f"Warning: Image file not found: {image_path}")
                # è¿”å›é»˜è®¤å›¾ç‰‡å’Œé®ç½©
                default_image = torch.zeros((1, height, width, 3), dtype=torch.float32)
                default_mask = torch.zeros((height, width), dtype=torch.float32, device="cpu")
                return default_image, default_mask
            
            # åŠ è½½å›¾ç‰‡
            i = Image.open(image_path)
            i = ImageOps.exif_transpose(i)
            image = i.convert("RGB")
            width_img, height_img = image.size
            image = np.array(image).astype(np.float32) / 255.0
            image = torch.from_numpy(image)[None,]

            # å¤„ç†é®ç½©
            if 'A' in i.getbands():
                mask = np.array(i.getchannel('A')).astype(np.float32) / 255.0
                mask = 1. - torch.from_numpy(mask)
                if mask.shape != (height_img, width_img):
                    mask = torch.nn.functional.interpolate(mask.unsqueeze(0).unsqueeze(0), 
                                                          size=(height_img, width_img), 
                                                          mode='bilinear', 
                                                          align_corners=False).squeeze()
            else:
                mask = torch.zeros((height_img, width_img), dtype=torch.float32, device="cpu")

            return image, mask
        except Exception as e:
            print(f"Warning: Failed to load image {image_path}: {e}")
            # è¿”å›é»˜è®¤å›¾ç‰‡å’Œé®ç½©
            default_image = torch.zeros((1, height, width, 3), dtype=torch.float32)
            default_mask = torch.zeros((height, width), dtype=torch.float32, device="cpu")
            return default_image, default_mask

    def load_video_file(self, video_path, frame_rate, max_frames, start_frame=0):
        """åŠ è½½è§†é¢‘"""
        try:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºç©º
            if not video_path or not video_path.strip() or video_path == "None":
                return (torch.zeros((1, 512, 512, 3)), 0, 512, 512)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(video_path):
                print(f"Warning: Video file not found: {video_path}")
                return (torch.zeros((1, 512, 512, 3)), 0, 512, 512)
            
            cap = cv2.VideoCapture(video_path)
            
            # æ£€æŸ¥è§†é¢‘æ˜¯å¦æˆåŠŸæ‰“å¼€
            if not cap.isOpened():
                print(f"Warning: Failed to open video file: {video_path}")
                cap.release()
                return (torch.zeros((1, 512, 512, 3)), 0, 512, 512)
            
            # è·å–è§†é¢‘çš„åŸå§‹å¸§ç‡å’Œæ€»å¸§æ•°
            original_fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # ç¡®ä¿start_frameä¸è¶…è¿‡æ€»å¸§æ•°
            start_frame = min(start_frame, total_frames - 1)
            
            # è®¾ç½®èµ·å§‹å¸§
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            # è®¡ç®—å¸§ç‡æ¯”ä¾‹
            fps_ratio = original_fps / frame_rate if frame_rate > 0 else 1
            
            # è®¡ç®—å®é™…æœ€å¤§å¸§æ•°
            remaining_frames = total_frames - start_frame
            max_frames = min(max_frames, remaining_frames)
            
            frames = []
            frame_idx = start_frame
            target_frame = 0
            
            # è¯»å–è§†é¢‘å¸§
            while frame_idx < total_frames and len(frames) < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æŒ‰ç…§ç›®æ ‡å¸§ç‡é‡‡æ ·
                if (frame_idx - start_frame) >= int(target_frame * fps_ratio):
                    # è½¬æ¢BGRä¸ºRGB
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    # å½’ä¸€åŒ–
                    frame = frame.astype(np.float32) / 255.0
                    frames.append(frame)
                    target_frame += 1
                
                frame_idx += 1
            
            cap.release()
            
            # è½¬æ¢ä¸ºå¼ é‡
            if frames:
                frames_tensor = torch.from_numpy(np.stack(frames, axis=0))
            else:
                # å¦‚æœæ²¡æœ‰è¯»å–åˆ°å¸§ï¼Œè¿”å›ä¸€ä¸ªç©ºçš„å¼ é‡
                frames_tensor = torch.zeros((1, 512, 512, 3), dtype=torch.float32)
            
            return (frames_tensor, len(frames), width, height)
        except Exception as e:
            print(f"Warning: Failed to load video {video_path}: {e}")
            return (torch.zeros((1, 512, 512, 3)), 0, 512, 512)

    def load_audio_file(self, audio_path):
        """åŠ è½½éŸ³é¢‘"""
        try:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºç©º
            if not audio_path or not audio_path.strip():
                return None
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(audio_path):
                print(f"Warning: Audio file not found: {audio_path}")
                return None
            
            waveform, sample_rate = torchaudio.load(audio_path)
            audio = {"waveform": waveform.unsqueeze(0), "sample_rate": sample_rate}
            # æ£€æŸ¥æ˜¯å¦æ˜¯å•å£°é“
            if waveform.shape[0] == 1:
                # å¤åˆ¶å•å£°é“ä¿¡å·åˆ°åŒå£°é“
                waveform = torch.cat([waveform, waveform], dim=0)
                audio["waveform"] = waveform.unsqueeze(0)
            return audio
        except Exception as e:
            print(f"Warning: Failed to load audio {audio_path}: {e}")
            return None

    def process_all_media(self, 
                         width=512, height=512, length=48, output_dir='',
                         text_input_a='', text_input_b='', text_input_c='', text_input_d='',
                         image_1_path='', image_2_path='', image_3_path='',
                         video_1_file='', video_1_frame_rate=24, video_1_max_frames=100, video_1_start_frame=0,
                         video_2_file='', video_2_frame_rate=24, video_2_max_frames=100, video_2_start_frame=0,
                         audio_main_path='', audio_alternate_path=''):
        """å¤„ç†æ‰€æœ‰åª’ä½“è¾“å…¥"""
        try:
            # æ–‡æœ¬å¤„ç†
            processed_text_a = text_input_a if text_input_a else ''
            processed_text_b = text_input_b if text_input_b else ''
            processed_text_c = text_input_c if text_input_c else ''
            processed_text_d = text_input_d if text_input_d else ''
            
            # å›¾ç‰‡å¤„ç†
            image_1, mask_1 = self.load_single_image(image_1_path, width, height)
            image_2, mask_2 = self.load_single_image(image_2_path, width, height)
            image_3, mask_3 = self.load_single_image(image_3_path, width, height)
            
            # è§†é¢‘å¤„ç† - è§†é¢‘1
            video_1_frames, _, _, _ = self.load_video_file(
                video_1_file, 
                video_1_frame_rate, video_1_max_frames, video_1_start_frame
            )
            
            # è§†é¢‘å¤„ç† - è§†é¢‘2
            video_2_frames, _, _, _ = self.load_video_file(
                video_2_file, 
                video_2_frame_rate, video_2_max_frames, video_2_start_frame
            )
            
            # éŸ³é¢‘å¤„ç†
            audio_main = self.load_audio_file(audio_main_path)
            audio_alternate = self.load_audio_file(audio_alternate_path)
            
            
            # è¿”å›æ‰€æœ‰ç»“æœ - æŒ‰ç…§RETURN_NAMESçš„é¡ºåº
            return (
                # å¸¸ç”¨å‚æ•°
                width, height, length, output_dir,
                # æ–‡æœ¬è¿”å›
                processed_text_a, processed_text_b, processed_text_c, processed_text_d,
                # å›¾ç‰‡è¿”å›
                image_1, mask_1, image_2, mask_2, image_3, mask_3,
                # è§†é¢‘è¿”å›
                video_1_frames,
                video_2_frames,
                # éŸ³é¢‘è¿”å›
                audio_main, audio_alternate
            )
        except Exception as e:
            print(f"Error in process_all_media: {e}")
            # è¿”å›é»˜è®¤å€¼
            default_image = torch.zeros((1, height, width, 3), dtype=torch.float32)
            default_mask = torch.zeros((height, width), dtype=torch.float32, device="cpu")
            # è®¡ç®—lengthï¼ˆå¯ä»¥ä½¿ç”¨heightä½œä¸ºé»˜è®¤å€¼ï¼‰
            length = height
            
            return (
                # å¸¸ç”¨å‚æ•°
                width, height, length,
                # æ–‡æœ¬è¿”å›
                '', '', '', '',
                # å›¾ç‰‡è¿”å›
                default_image, default_mask, default_image, default_mask, default_image, default_mask,
                # è§†é¢‘è¿”å›
                torch.zeros((1, 512, 512, 3)),
                torch.zeros((1, 512, 512, 3)),
                # éŸ³é¢‘è¿”å›
                None, None
            )


# èŠ‚ç‚¹æ˜ å°„å­—å…¸
NODE_CLASS_MAPPINGS = {
    "FTKSaveImage": FTKSaveImage,   
    "FTKSaveVideo": FTKSaveVideo,    
    "FTKSaveText": FTKSaveText,
    "FTK_OUTPUT": FTK_OUTPUT,    
    "FTKSaveAudio": FTKSaveAudio,
    "FTK_INPUT": FTK_INPUT,
}
